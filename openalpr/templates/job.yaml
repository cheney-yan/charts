{{- $type := .Values.type -}}
{{- $parallelism := .Values.parallelism -}}
{{- $cpu := .Values.resources.requests.cpu -}}
{{- $memory := .Values.resources.requests.memory -}}
{{- $maxCpu := .Values.resources.max.cpu -}}
{{- $requests := .Values.resources.requests -}}
{{- $burst := .Values.burst -}}
{{- $release := .Release.name -}}
{{- $dockerImage := .Values.image -}}
{{- $gpuType := .Values.gpuType -}}
{{- $imageFolder := .Values.imageFolder -}}
{{- $storname := .Values.storage.name -}}
{{- $command := .Values.command -}}

---
# {{- range $index, $benchmark := .Values.benchmarks }}
{{- range $job, $nb := until (int .Values.parallelism) }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $type | lower }}-{{ $parallelism }}-{{ $cpu | lower }}-{{ $memory | lower }}-{{ $gpuType | lower }}-{{ $job }}
spec:
  parallelism: 1
  template:
    metadata:
      labels:
        app: {{ template "fullname" . }}
    spec:
      containers:
      - name: {{ $benchmark | replace "/" "-" | lower }}-{{ $job }}
        image: {{- $dockerImage -}}:{{- if eq $gpuType "nvidia" -}}latest-3.3.1-16.04-cuda-2{{- else -}}latest-3.3.1-16.04-nocuda{{- end -}}
        command: {{ $command }}
        env:
        - name: JOB_NAME
          valueFrom:
            fieldRef:
              fieldPath: {{ $type | lower }}-{{ $parallelism }}-{{ $cpu | lower }}-{{ $memory | lower }}-{{ $gpuType | lower }}-{{ $job }}
       {{ if eq $gpuType "nvidia" }}
        - name: LD_LIBRARY_PATH
          value: "$LD_LIBRARY_PATH:/usr/lib/nvidia:/usr/lib/cuda"
        {{ end }}
        # {{ if eq $gpuType "amd" }}
        # - name: LD_LIBRARY_PATH
        #   value: "$LD_LIBRARY_PATH:/opt/amdgpu-pro/lib:/usr/lib/x86_64-linux-gnu"
        # {{ end }}
        volumeMounts:
        - name: {{ $release }}-config
          mountPath: /etc/openalpr
        - name: {{ $storname }}
          mountPath: /data
        {{ if eq $gpuType "nvidia" }}
        - mountPath: /usr/local/nvidia/bin
          name: bin
        - mountPath: /usr/lib/nvidia
          name: lib
        - mountPath: /usr/lib/cuda
          name: libcuda
        {{ end }}
        # {{ if eq $gpuType "amd" }}
        # - mountPath: /dev/dri
        #   name: dri
        # {{ end }}
        resources:
          requests: 
{{ toYaml $requests | indent 12 }}
            {{ if eq $gpuType "nvidia" }}
            alpha.kubernetes.io/nvidia-gpu: 1
            {{ end }}
          limits:
            # cpu: {{ if $burst }}{{ max (mul 2 (atoi $cpu)) (atoi $maxCpu) | quote }}{{ else }}{{ $cpu }}{{ end }}
            cpu: {{ if $burst }}{{  max (atoi $cpu) (atoi $maxCpu) }}{{ else }}{{ $cpu }}{{ end }}
            memory: {{ $memory }}
            {{ if eq $gpuType "nvidia" }}
            alpha.kubernetes.io/nvidia-gpu: 1
            {{ end }}
      restartPolicy: Never
      volumes:
      - name: {{ $release }}-config
        configMap:
          name: {{ $release }}-config
      - name: {{ $storname }}
        persistentVolumeClaim:
          claimName: {{ $storname }}
      {{ if eq $gpuType "nvidia" }}
      - name: bin
        hostPath:
          path: /usr/lib/nvidia-384/bin
      - name: lib
        hostPath:
          path: /usr/lib/nvidia-384
      - name: libcuda
        hostPath:
          path: /usr/lib/x86_64-linux-gnu
      {{ end }}
      # {{ if eq $gpuType "amd" }}
      # - name: dri
      #   hostPath:
      #     path: /dev/dri
      # {{ end }}
---
{{- end }}
# {{- end }}
